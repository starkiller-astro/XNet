{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 620,
   "id": "6e1d3539-b463-443b-9df9-481dd22bccf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This script incorporates an autoencoder, leveraging input data for the autoencoder (AE) and subsequently conducting a principal component analysis to evaluate the outcomes.\n",
    "'''\n",
    "import torch\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import transforms\n",
    "from sklearn.model_selection import train_test_split  # Import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "id": "382c4920-6fdb-4589-9a98-f30ae787e0b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#class CustomVariableInputDataset_2(Dataset):\n",
    "#    def __init__(self, csv_file):\n",
    "#        # Skip the first row containing non-numeric values\n",
    "#        data = pd.read_csv(csv_file, skiprows=[0], header=None)\n",
    "#        self.data = data.values.astype(np.float32)\n",
    "#        non_zero_indices = self.data > 0\n",
    "#        self.data[non_zero_indices] = np.log10(self.data[non_zero_indices])\n",
    "#        self.scaler = MinMaxScaler()\n",
    "#        self.data = self.scaler.fit_transform(self.data)\n",
    "#        self.data = self.scaler.inverse_transform(self.data)\n",
    "#        self.data = 10 ** self.data\n",
    "#\n",
    "#    def __len__(self):\n",
    "#        return len(self.data)\n",
    "#\n",
    "#    def __getitem__(self, idx):\n",
    "#        sample = self.data[idx]\n",
    "#        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "id": "9c4e8cd4-270d-4f8f-90f8-8d4f9ca8e745",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomVariableInputDataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        # Skip the first row containing non-numeric values\n",
    "        data = pd.read_csv(csv_file, skiprows=[0], header=None)\n",
    "        self.data = data.values.astype(np.float32)\n",
    "        self.data = np.log10(self.data)\n",
    "        self.scaler = MinMaxScaler()\n",
    "        self.data = self.scaler.fit_transform(self.data)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "id": "513bb2a6-a5d7-455b-bbc5-9abee07b9c5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 2: Define Data Transformations # not aplicable for this project \n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "id": "94ad84c0-b54f-44f6-b78e-264ab29587c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 3: Instantiate Your Custom Variable Input Dataset\n",
    "csv_file_path = '/Users/alancangas/Xnet/test/x_log_training_output.csv'\n",
    "custom_variable_input_dataset = CustomVariableInputDataset(csv_file_path)\n",
    "#custom_variable_input_dataset_2 = CustomVariableInputDataset_2(csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "id": "39791881-c879-49ea-82f0-4fa32124e3ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 3303106\n",
      "First few samples:\n",
      "log and scaled\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of samples:\", len(custom_variable_input_dataset))\n",
    "print(\"First few samples:\")\n",
    "for i in range(1): \n",
    "    print('log and scaled')\n",
    "    print(custom_variable_input_dataset[i])\n",
    "    #print(custom_variable_input_dataset_2[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "id": "0a196aac-1329-40a2-ab24-b7864dfa2c0d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9078262  0.9937367  0.9816858  0.96454465 0.9508282  0.9029752\n",
      " 0.84939796 0.75768363 0.60696137 0.49136055 0.21487063 0.\n",
      " 0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Split Data into Training and Testing Sets\n",
    "X_train, X_test = train_test_split(custom_variable_input_dataset, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "id": "df6ddf4e-261e-4ff8-af98-1dd66f4be94b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define DataLoaders for both training and testing sets\n",
    "batch_size = 32 #Control Batch size : 32\n",
    "train_dataloader = DataLoader(X_train, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(X_test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "id": "55d01ff8-f845-4209-b26f-4f3f34b3e18e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 5: Define the Autoencoder Model\n",
    "class VariableInputAutoencoder(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(VariableInputAutoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 12),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(12, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4, 2)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(2, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 12),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(12, input_size),  # Output size must match input size\n",
    "            nn.ReLU() # replace with ReLU for -log(x)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "id": "4738959d-84a5-4c59-9b6d-538dc4d02eae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 6: Instantiate the Autoencoder Model with an Appropriate Input Size\n",
    "input_size = len(custom_variable_input_dataset[0])  # Determine the input size dynamically\n",
    "model = VariableInputAutoencoder(input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "id": "9f928711-6945-4de9-bd45-8a4953453798",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 7: Define Training Parameters and Train the Autoencoder\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001 #control learning rate 0.001\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "id": "5965cb9f-3d7a-4828-835c-6a743e5f4a40",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Training Loss: 0.28778756\n",
      "Testing Loss: 0.28542992\n",
      "Epoch [2/10], Training Loss: 0.24244355\n",
      "Testing Loss: 0.23880636\n",
      "Epoch [3/10], Training Loss: 0.17925207\n",
      "Testing Loss: 0.16793109\n",
      "Epoch [4/10], Training Loss: 0.15599847\n",
      "Testing Loss: 0.15074926\n",
      "Epoch [5/10], Training Loss: 0.14339384\n",
      "Testing Loss: 0.14779282\n",
      "Epoch [6/10], Training Loss: 0.13666169\n",
      "Testing Loss: 0.14673367\n",
      "Epoch [7/10], Training Loss: 0.14448139\n",
      "Testing Loss: 0.14603965\n",
      "Epoch [8/10], Training Loss: 0.14999712\n",
      "Testing Loss: 0.14571310\n",
      "Epoch [9/10], Training Loss: 0.15456361\n",
      "Testing Loss: 0.14554310\n",
      "Epoch [10/10], Training Loss: 0.14921314\n",
      "Testing Loss: 0.14546254\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = criterion(output, batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {loss.item():.8f}')\n",
    "    # Move this loop into the epochs to print both and compare\n",
    "    # Testing loop\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            output = model(batch)\n",
    "            loss = criterion(output, batch)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    print(f'Testing Loss: {test_loss / len(test_dataloader):.8f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "id": "3c41b94f-c695-4f33-aba8-d5097f827658",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---Processed Input---\n",
      "\n",
      "tensor([0.7988, 0.8527, 1.0000, 0.8658, 0.8527, 0.8366, 0.7890, 0.7075, 0.5710,\n",
      "        0.4677, 0.2131, 0.0000, 0.0000, 0.0000])\n",
      "\n",
      "---Reconstructed Processed Input---\n",
      "\n",
      "tensor([0.0000, 0.9590, 0.9908, 0.0000, 0.9488, 0.8881, 0.8264, 0.7289, 0.5629,\n",
      "        0.0000, 0.1778, 0.0000, 0.0000, 0.0000], grad_fn=<ReluBackward0>)\n",
      "\n",
      "---Processed Input---\n",
      "\n",
      "tensor([0.7999, 0.8539, 1.0000, 0.8669, 0.8543, 0.8379, 0.7907, 0.7093, 0.5727,\n",
      "        0.4697, 0.2149, 0.0000, 0.0000, 0.0000])\n",
      "\n",
      "---Reconstructed Processed Input---\n",
      "\n",
      "tensor([0.0000, 0.9587, 0.9908, 0.0000, 0.9495, 0.8890, 0.8278, 0.7308, 0.5651,\n",
      "        0.0000, 0.1803, 0.0000, 0.0000, 0.0000], grad_fn=<ReluBackward0>)\n",
      "\n",
      "---Processed Input---\n",
      "\n",
      "tensor([0.8008, 0.8550, 1.0000, 0.8678, 0.8558, 0.8390, 0.7924, 0.7111, 0.5746,\n",
      "        0.4718, 0.2169, 0.0000, 0.0000, 0.0000])\n",
      "\n",
      "---Reconstructed Processed Input---\n",
      "\n",
      "tensor([0.0000, 0.9584, 0.9909, 0.0000, 0.9502, 0.8899, 0.8292, 0.7328, 0.5673,\n",
      "        0.0000, 0.1828, 0.0000, 0.0000, 0.0000], grad_fn=<ReluBackward0>)\n",
      "\n",
      "---Processed Input---\n",
      "\n",
      "tensor([0.8016, 0.8560, 1.0000, 0.8686, 0.8573, 0.8402, 0.7941, 0.7130, 0.5765,\n",
      "        0.4740, 0.2191, 0.0000, 0.0000, 0.0000])\n",
      "\n",
      "---Reconstructed Processed Input---\n",
      "\n",
      "tensor([0.0000, 0.9581, 0.9909, 0.0000, 0.9509, 0.8909, 0.8308, 0.7348, 0.5697,\n",
      "        0.0000, 0.1855, 0.0000, 0.0000, 0.0000], grad_fn=<ReluBackward0>)\n",
      "\n",
      "---Processed Input---\n",
      "\n",
      "tensor([0.8024, 0.8569, 1.0000, 0.8693, 0.8588, 0.8412, 0.7958, 0.7150, 0.5786,\n",
      "        0.4764, 0.2213, 0.0000, 0.0000, 0.0000])\n",
      "\n",
      "---Reconstructed Processed Input---\n",
      "\n",
      "tensor([0.0000, 0.9577, 0.9909, 0.0000, 0.9517, 0.8919, 0.8323, 0.7370, 0.5723,\n",
      "        0.0000, 0.1884, 0.0000, 0.0000, 0.0000], grad_fn=<ReluBackward0>)\n",
      "\n",
      "---Processed Input---\n",
      "\n",
      "tensor([0.8030, 0.8578, 1.0000, 0.8700, 0.8603, 0.8423, 0.7976, 0.7171, 0.5808,\n",
      "        0.4790, 0.2238, 0.0000, 0.0000, 0.0000])\n",
      "\n",
      "---Reconstructed Processed Input---\n",
      "\n",
      "tensor([0.0000, 0.9573, 0.9909, 0.0000, 0.9525, 0.8930, 0.8340, 0.7393, 0.5749,\n",
      "        0.0000, 0.1914, 0.0000, 0.0000, 0.0000], grad_fn=<ReluBackward0>)\n",
      "\n",
      "---Processed Input---\n",
      "\n",
      "tensor([0.8036, 0.8586, 1.0000, 0.8705, 0.8618, 0.8434, 0.7994, 0.7194, 0.5832,\n",
      "        0.4817, 0.2264, 0.0000, 0.0000, 0.0000])\n",
      "\n",
      "---Reconstructed Processed Input---\n",
      "\n",
      "tensor([0.0000, 0.9570, 0.9909, 0.0000, 0.9534, 0.8941, 0.8358, 0.7417, 0.5777,\n",
      "        0.0000, 0.1945, 0.0000, 0.0000, 0.0000], grad_fn=<ReluBackward0>)\n",
      "\n",
      "---Processed Input---\n",
      "\n",
      "tensor([0.8041, 0.8594, 1.0000, 0.8710, 0.8633, 0.8444, 0.8013, 0.7217, 0.5857,\n",
      "        0.4846, 0.2292, 0.0000, 0.0000, 0.0000])\n",
      "\n",
      "---Reconstructed Processed Input---\n",
      "\n",
      "tensor([0.0000, 0.9565, 0.9910, 0.0000, 0.9543, 0.8953, 0.8377, 0.7442, 0.5807,\n",
      "        0.0000, 0.1979, 0.0000, 0.0000, 0.0000], grad_fn=<ReluBackward0>)\n",
      "\n",
      "---Processed Input---\n",
      "\n",
      "tensor([0.8046, 0.8602, 1.0000, 0.8715, 0.8648, 0.8455, 0.8032, 0.7241, 0.5883,\n",
      "        0.4878, 0.2323, 0.0000, 0.0000, 0.0000])\n",
      "\n",
      "---Reconstructed Processed Input---\n",
      "\n",
      "tensor([0.0000, 0.9561, 0.9910, 0.0000, 0.9553, 0.8966, 0.8397, 0.7470, 0.5839,\n",
      "        0.0000, 0.2015, 0.0000, 0.0000, 0.0000], grad_fn=<ReluBackward0>)\n",
      "\n",
      "---Processed Input---\n",
      "\n",
      "tensor([0.8050, 0.8610, 1.0000, 0.8719, 0.8662, 0.8466, 0.8051, 0.7266, 0.5911,\n",
      "        0.4911, 0.2357, 0.0000, 0.0000, 0.0000])\n",
      "\n",
      "---Reconstructed Processed Input---\n",
      "\n",
      "tensor([0.0000, 0.9556, 0.9910, 0.0000, 0.9563, 0.8979, 0.8419, 0.7498, 0.5873,\n",
      "        0.0000, 0.2053, 0.0000, 0.0000, 0.0000], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(100,110):\n",
    "    original_input = torch.from_numpy(custom_variable_input_dataset[i]) ## deprocess\n",
    "    inverse_transformed_original_input = custom_variable_input_dataset.scaler.inverse_transform(original_input.detach().numpy().reshape([1,-1]))\n",
    "    reconstructed_original_input = 10 ** inverse_transformed_original_input\n",
    "    ae_reconstructed_input = model.forward(original_input)\n",
    "    print('')\n",
    "    print('---Processed Input---')\n",
    "    print('')\n",
    "    print(original_input)\n",
    "    print('')\n",
    "    print('---Reconstructed Processed Input---')\n",
    "    print('')\n",
    "    print(ae_reconstructed_input)\n",
    "    #print('')\n",
    "    #print('---Pre-processed Input---')\n",
    "    #print('')\n",
    "    #print(reconstructed_original_input)\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "id": "43d209f3-d623-4f6d-bb48-300d0b52c4a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA Reconstruction Error: 0.24286094\n",
      "PCA Reconstruction Error: 7.02149324\n"
     ]
    }
   ],
   "source": [
    "# Desired lower dimensionality\n",
    "n_components = 2 \n",
    "\n",
    "# Extract the training data\n",
    "X_train = [] # log # then standardize\n",
    "for batch in train_dataloader:\n",
    "    X_train.append(batch.numpy())\n",
    "\n",
    "# Concatenate the training data batches into a single array\n",
    "X_train = np.concatenate(X_train, axis=0)\n",
    "\n",
    "# Extract the test data from DataLoader\n",
    "X_test = [] # stay as it is\n",
    "for batch in test_dataloader:\n",
    "    X_test.append(batch.numpy())\n",
    "\n",
    "# Concatenate the test data batches into a single array\n",
    "X_test = np.concatenate(X_test, axis=0)\n",
    "inverse_transformed_X_test = custom_variable_input_dataset.scaler.inverse_transform(X_test)\n",
    "X_test = 10 ** inverse_transformed_X_test\n",
    "\n",
    "\n",
    "\n",
    "# Create and fit PCA\n",
    "pca = PCA(n_components=n_components)\n",
    "pca.fit(X_train)\n",
    "\n",
    "# Transform test data to the lower-dimensional space\n",
    "data_pca = pca.transform(X_test)\n",
    "\n",
    "# Reconstruct data from the lower-dimensional space\n",
    "data_pca_reconstructed = pca.inverse_transform(data_pca)\n",
    "# de standarized the reconstructed data and \"de log it\"\n",
    "# Calculate PCA reconstruction error\n",
    "pca_reconstruction_error = mean_squared_error(X_test, data_pca_reconstructed)\n",
    "pca_rel_reconstruction_error = mean_squared_error(X_test, data_pca_reconstructed) / mean_squared_error(X_test, np.zeros(X_test.shape))\n",
    " \n",
    "print(f\"PCA Reconstruction Error: {pca_reconstruction_error:.8f}\")\n",
    "print(f\"PCA Reconstruction Error: {pca_rel_reconstruction_error:.8f}\")\n",
    "# 2 test data, log , normal for comparizon\n",
    "# log_X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "id": "2e950914-63c4-4964-9837-ccaddda35cc1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction Error using 2 Principal Components: 0.03227447\n"
     ]
    }
   ],
   "source": [
    "# first step convert the data\n",
    "# has to be normalized \"standradize\", why? \n",
    "# all my zeros in my data convert them into 10^-30\n",
    "# log 10 \n",
    "#column wise, 0 mean with standard deviation \n",
    "# Desired lower dimensionality \n",
    "# when interpreting the results \"de-log it\"\n",
    "n_components = 2 \n",
    "\n",
    "# Compute the covariance matrix\n",
    "cov_matrix = np.cov(X_train, rowvar=False)\n",
    "\n",
    "# Perform eigenvalue decomposition\n",
    "eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n",
    "\n",
    "# Sort eigenvalues and eigenvectors in descending order\n",
    "sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "eigenvalues = eigenvalues[sorted_indices]\n",
    "eigenvectors = eigenvectors[:, sorted_indices]\n",
    "\n",
    "# Select the top N principal components\n",
    "selected_eigenvectors = eigenvectors[:, :n_components]\n",
    "\n",
    "# Project the data onto the selected principal components\n",
    "projected_data = np.dot(X_test, selected_eigenvectors)\n",
    "\n",
    "# Reconstruct data from the lower-dimensional space\n",
    "reconstructed_data = np.dot(projected_data, selected_eigenvectors.T)\n",
    "\n",
    "# Calculate the reconstruction error\n",
    "reconstruction_error = np.mean(np.square(X_test - reconstructed_data))\n",
    "\n",
    "print(f\"Reconstruction Error using {n_components} Principal Components: {reconstruction_error:.8f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33a4ac76-235e-4e91-9a66-25b5dea1295c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/alancangas'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a160fb4-eaf5-4982-8fb6-a4dbaba9ac40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
